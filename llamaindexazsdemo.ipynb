{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Notebook to accompany Microsoft AI Search's \"Advanced RAG with Azure AI Search and LlamaIndex\" Blog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Environment Prerequisites prior to running this notebook\n",
    "\n",
    "##Provision Azure Open AI Service:https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/create-resource\n",
    "    #Create an embeddings deployment (text-embeddings-ada002): https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#embeddings-models\n",
    "    #Create an LLM model deployment (gpt-4): https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#gpt-4-and-gpt-4-turbo-preview\n",
    "    #Once the deployments are created, take note of the endpoint and key for the service and the deployment names for each model\n",
    "\n",
    "##Provision an Azure AI Search Instance: https://learn.microsoft.com/en-us/azure/search/search-create-service-portal\n",
    "    #Choose the tier with semantic ranker enabled\n",
    "\n",
    "##Install Python 3.8 or later: https://www.python.org/downloads/\n",
    "\n",
    "##Install Jupyter Notebook: https://jupyter.org/install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Install required packages\n",
    "%pip install ipykernel azure-search-documents llama-index llama-index-vector-stores-azureaisearch llama-index-embeddings-azure-openai azure-identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modules\n",
    "import os\n",
    "from IPython.display import Markdown, display\n",
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.vector_stores.azureaisearch import AzureAISearchVectorStore, IndexManagement, MetadataIndexFieldType\n",
    "from llama_index.core import StorageContext, VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core import Settings\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.core.credentials import AzureKeyCredential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#***Fill-in Azure Open AI Variables****\n",
    "#Obtain Azure OpenAI (AOAI) key & endpoint in the Azure Portal -> Azure OpenAI -> AOAI Instance -> Resource Management -> Keys and Endpoint\n",
    "AOAI_API_KEY = 'your alphanumeric AOAI key'\n",
    "AOAI_ENDPOINT = 'https://youraoai.openai.azure.com/'\n",
    "\n",
    "#AOAI api version below should work for gpt-4 models\n",
    "AOAI_API_VERSION = '2024-02-15-preview'\n",
    "\n",
    "#Change if you want to use a different model, ensure you use the 'model name' and not the 'deployment name' from the Azure OpenAI Service\n",
    "AOAI_LLM_MODEL = 'gpt-4'\n",
    "\n",
    "#Change the deployment name of your AOAI llm model if not the same as the model name\n",
    "AOAI_LLM_DEPLOYMENT_NAME = \"gpt-4\"\n",
    "\n",
    "#The Azure OpenAI Embedding model is a separate model that is used to convert text to embeddings, usually text-embedding-ada-002\n",
    "AOAI_EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "\n",
    "#Change to the 'deployment name' of your embeddings model if not same as the model name\n",
    "AOAI_EMBEDDING_DEPLOYMENT_NAME = \"text-embedding-ada-002\"\n",
    "\n",
    "#***Create Azure AI Search Variables\n",
    "#Obtain from Azure Portal -> AI Search -> Settings -> Keys\n",
    "SEARCH_SERVICE_API_KEY = \"your AI Search key\"\n",
    "#Obtain from Azure Portal -> AI Search -> Overview -> Url \n",
    "SEARCH_SERVICE_ENDPOINT = \"https://youraisearchurl.search.windows.net\"\n",
    "#If needed to change, versions table here: https://learn.microsoft.com/en-us/rest/api/searchservice/search-service-api-versions#stable-versions\n",
    "SEARCH_SERVICE_API_VERSION = \"2023-11-01\"\n",
    "CREDENTIAL = AzureKeyCredential(SEARCH_SERVICE_API_KEY)\n",
    "#Name the index to be created in AI Search (lowercase, digits or dashes only)\n",
    "INDEX_NAME = \"llamaindex-vector-demo\"\n",
    "###*****End of Variables*****####\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "    model=AOAI_LLM_MODEL,\n",
    "    deployment_name=AOAI_LLM_DEPLOYMENT_NAME,\n",
    "    api_key=AOAI_API_KEY,\n",
    "    azure_endpoint=AOAI_ENDPOINT,\n",
    "    api_version=AOAI_API_VERSION,\n",
    ")\n",
    "\n",
    "embed_model = AzureOpenAIEmbedding(\n",
    "    model=AOAI_EMBEDDING_MODEL,\n",
    "    deployment_name=AOAI_EMBEDDING_DEPLOYMENT_NAME,\n",
    "    api_key=AOAI_API_KEY,\n",
    "    azure_endpoint=AOAI_ENDPOINT,\n",
    "    api_version=AOAI_API_VERSION,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "###AI Search Configuration\n",
    "\n",
    "# Use index client to demonstrate creating an index\n",
    "index_client = SearchIndexClient(\n",
    "    endpoint=SEARCH_SERVICE_ENDPOINT,\n",
    "    credential=CREDENTIAL,\n",
    ")\n",
    "\n",
    "# Use search client to demonstration using existing index\n",
    "search_client = SearchClient(\n",
    "    endpoint=SEARCH_SERVICE_ENDPOINT,\n",
    "    index_name=INDEX_NAME,\n",
    "    credential=CREDENTIAL,\n",
    ")\n",
    "\n",
    "metadata_fields = {\n",
    "    \"author\": \"author\",\n",
    "    \"theme\": (\"topic\", MetadataIndexFieldType.STRING),\n",
    "    \"director\": \"director\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Vector store configuration\n",
    "vector_store = AzureAISearchVectorStore(\n",
    "    search_or_index_client=index_client,\n",
    "    filterable_metadata_field_keys=metadata_fields,\n",
    "    index_name=INDEX_NAME,\n",
    "    index_management=IndexManagement.CREATE_IF_NOT_EXISTS,\n",
    "    id_field_key=\"id\",\n",
    "    chunk_field_key=\"chunk\",\n",
    "    embedding_field_key=\"embedding\",\n",
    "    embedding_dimensionality=1536,\n",
    "    metadata_string_field_key=\"metadata\",\n",
    "    doc_id_field_key=\"doc_id\",\n",
    "    language_analyzer=\"en.lucene\",\n",
    "    vector_algorithm_type=\"exhaustiveKnn\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Automatically download and index the sample dataset to the working directory\n",
    "import requests\n",
    "url = 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt'\n",
    "response = requests.get(url)\n",
    "with open('paul_graham_essay.txt', 'wb') as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "reader = SimpleDirectoryReader(input_files=[\"paul_graham_essay.txt\"])\n",
    "documents = reader.load_data()\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The author worked on several different things throughout his life. Before college, he worked on writing short stories and programming. His first programming experiences were with an IBM 1401 using punch cards and Fortran. Later, he worked with microcomputers, writing simple games, a program to predict model rocket flights, and a word processor his father used for writing a book.\n",
       "\n",
       "In college, he initially planned to study philosophy but switched to AI, inspired by a novel by Heinlein and a PBS documentary featuring SHRDLU. He also worked on Lisp, creating a new dialect called Arc, and wrote essays that he published online, which gained significant attention after being shared on Slashdot. He wrote a book about Lisp hacking titled \"On Lisp,\" and engaged in systems work, desiring to build things that would last.\n",
       "\n",
       "Additionally, he worked on spam filters, did some painting, hosted dinners, and bought a building in Cambridge. He also contemplated a career in art after realizing paintings could be a lasting form of creation, taking art classes while still in a PhD program for computer science. Ultimately, he completed his dissertation in a short time frame after being asked if he could graduate that June."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Query Data - First query can take up to 5 minutes depending on the size of the dataset to index\n",
    "query_engine = index.as_query_engine(similarity_top_k=3)\n",
    "response = query_engine.query(\"What did the author work on?\")\n",
    "display(Markdown(f\"{response}\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

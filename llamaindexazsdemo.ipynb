{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Environment Prerequisites prior to running this notebook\n",
    "\n",
    "##Provision Azure Open AI Service:https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/create-resource\n",
    "    #Create an embeddings deployment (text-embeddings-ada002): https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#embeddings-models\n",
    "    #Create an LLM model deployment (gpt-4): https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#gpt-4-and-gpt-4-turbo-preview\n",
    "    #Once the deployments are created, take note of the endpoint and key for the service and the deployment names for each model\n",
    "\n",
    "##Provision an Azure AI Search Instance: https://learn.microsoft.com/en-us/azure/search/search-create-service-portal\n",
    "    #Choose the tier with semantic ranker enabled\n",
    "\n",
    "##Install Python 3.8 or later: https://www.python.org/downloads/\n",
    "\n",
    "##Install Jupyter Notebook: https://jupyter.org/install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipykernel in c:\\users\\moapurva\\documents\\github\\azure-search-vector-samples\\.venv\\lib\\site-packages (6.29.4)\n",
      "Collecting azure-search-documents\n",
      "  Using cached azure_search_documents-11.4.0-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting llama-index\n",
      "  Using cached llama_index-0.10.30-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting llama-index-vector-stores-azureaisearch\n",
      "  Using cached llama_index_vector_stores_azureaisearch-0.1.4-py3-none-any.whl.metadata (743 bytes)\n",
      "Collecting llama-index-embeddings-azure-openai\n",
      "  Using cached llama_index_embeddings_azure_openai-0.1.7-py3-none-any.whl.metadata (752 bytes)\n",
      "Collecting azure-identity\n",
      "  Using cached azure_identity-1.16.0-py3-none-any.whl.metadata (76 kB)\n",
      "Requirement already satisfied: comm>=0.1.1 in c:\\users\\moapurva\\documents\\github\\azure-search-vector-samples\\.venv\\lib\\site-packages (from ipykernel) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\moapurva\\documents\\github\\azure-search-vector-samples\\.venv\\lib\\site-packages (from ipykernel) (1.8.1)\n",
      "Requirement already satisfied: ipython>=7.23.1 in c:\\users\\moapurva\\documents\\github\\azure-search-vector-samples\\.venv\\lib\\site-packages (from ipykernel) (8.23.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\moapurva\\documents\\github\\azure-search-vector-samples\\.venv\\lib\\site-packages (from ipykernel) (8.6.1)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\moapurva\\documents\\github\\azure-search-vector-samples\\.venv\\lib\\site-packages (from ipykernel) (5.7.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\moapurva\\documents\\github\\azure-search-vector-samples\\.venv\\lib\\site-packages (from ipykernel) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\moapurva\\documents\\github\\azure-search-vector-samples\\.venv\\lib\\site-packages (from ipykernel) (1.6.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\moapurva\\documents\\github\\azure-search-vector-samples\\.venv\\lib\\site-packages (from ipykernel) (24.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\moapurva\\documents\\github\\azure-search-vector-samples\\.venv\\lib\\site-packages (from ipykernel) (5.9.8)\n",
      "Requirement already satisfied: pyzmq>=24 in c:\\users\\moapurva\\documents\\github\\azure-search-vector-samples\\.venv\\lib\\site-packages (from ipykernel) (26.0.0)\n",
      "Requirement already satisfied: tornado>=6.1 in c:\\users\\moapurva\\documents\\github\\azure-search-vector-samples\\.venv\\lib\\site-packages (from ipykernel) (6.4)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in c:\\users\\moapurva\\documents\\github\\azure-search-vector-samples\\.venv\\lib\\site-packages (from ipykernel) (5.14.2)\n",
      "Collecting azure-core<2.0.0,>=1.28.0 (from azure-search-documents)\n",
      "  Using cached azure_core-1.30.1-py3-none-any.whl.metadata (37 kB)\n",
      "Collecting azure-common~=1.1 (from azure-search-documents)\n",
      "  Using cached azure_common-1.1.28-py2.py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting isodate>=0.6.0 (from azure-search-documents)\n",
      "  Using cached isodate-0.6.1-py2.py3-none-any.whl.metadata (9.6 kB)\n",
      "Collecting llama-index-agent-openai<0.3.0,>=0.1.4 (from llama-index)\n",
      "  Using cached llama_index_agent_openai-0.2.2-py3-none-any.whl.metadata (677 bytes)\n",
      "Collecting llama-index-cli<0.2.0,>=0.1.2 (from llama-index)\n",
      "  Using cached llama_index_cli-0.1.12-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting llama-index-core<0.11.0,>=0.10.30 (from llama-index)\n",
      "  Using cached llama_index_core-0.10.30-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting llama-index-embeddings-openai<0.2.0,>=0.1.5 (from llama-index)\n",
      "  Using cached llama_index_embeddings_openai-0.1.7-py3-none-any.whl.metadata (603 bytes)\n",
      "Collecting llama-index-indices-managed-llama-cloud<0.2.0,>=0.1.2 (from llama-index)\n",
      "  Using cached llama_index_indices_managed_llama_cloud-0.1.5-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index)\n",
      "  Using cached llama_index_legacy-0.9.48-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting llama-index-llms-openai<0.2.0,>=0.1.13 (from llama-index)\n",
      "  Using cached llama_index_llms_openai-0.1.15-py3-none-any.whl.metadata (559 bytes)\n",
      "Collecting llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 (from llama-index)\n",
      "  Using cached llama_index_multi_modal_llms_openai-0.1.5-py3-none-any.whl.metadata (677 bytes)\n",
      "Collecting llama-index-program-openai<0.2.0,>=0.1.3 (from llama-index)\n",
      "  Using cached llama_index_program_openai-0.1.5-py3-none-any.whl.metadata (715 bytes)\n",
      "Collecting llama-index-question-gen-openai<0.2.0,>=0.1.2 (from llama-index)\n",
      "  Using cached llama_index_question_gen_openai-0.1.3-py3-none-any.whl.metadata (785 bytes)\n",
      "Collecting llama-index-readers-file<0.2.0,>=0.1.4 (from llama-index)\n",
      "  Using cached llama_index_readers_file-0.1.19-py3-none-any.whl.metadata (979 bytes)\n",
      "Collecting llama-index-readers-llama-parse<0.2.0,>=0.1.2 (from llama-index)\n",
      "  Using cached llama_index_readers_llama_parse-0.1.4-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting llama-index-llms-azure-openai<0.2.0,>=0.1.3 (from llama-index-embeddings-azure-openai)\n",
      "  Using cached llama_index_llms_azure_openai-0.1.6-py3-none-any.whl.metadata (735 bytes)\n",
      "Collecting cryptography>=2.5 (from azure-identity)\n",
      "  Using cached cryptography-42.0.5-cp39-abi3-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting msal>=1.24.0 (from azure-identity)\n",
      "  Using cached msal-1.28.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting msal-extensions>=0.3.0 (from azure-identity)\n",
      "  Using cached msal_extensions-1.1.0-py3-none-any.whl.metadata (7.7 kB)\n",
      "Collecting requests>=2.21.0 (from azure-core<2.0.0,>=1.28.0->azure-search-documents)\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\moapurva\\documents\\github\\azure-search-vector-samples\\.venv\\lib\\site-packages (from azure-core<2.0.0,>=1.28.0->azure-search-documents) (1.16.0)\n",
      "Collecting typing-extensions>=4.6.0 (from azure-core<2.0.0,>=1.28.0->azure-search-documents)\n",
      "  Using cached typing_extensions-4.11.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting cffi>=1.12 (from cryptography>=2.5->azure-identity)\n",
      "  Using cached cffi-1.16.0-cp312-cp312-win_amd64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: decorator in c:\\users\\moapurva\\documents\\github\\azure-search-vector-samples\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\moapurva\\documents\\github\\azure-search-vector-samples\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (0.19.1)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\moapurva\\documents\\github\\azure-search-vector-samples\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\moapurva\\documents\\github\\azure-search-vector-samples\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (2.17.2)\n",
      "Requirement already satisfied: stack-data in c:\\users\\moapurva\\documents\\github\\azure-search-vector-samples\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (0.6.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\moapurva\\documents\\github\\azure-search-vector-samples\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\moapurva\\documents\\github\\azure-search-vector-samples\\.venv\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\moapurva\\documents\\github\\azure-search-vector-samples\\.venv\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel) (4.2.0)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\moapurva\\documents\\github\\azure-search-vector-samples\\.venv\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel) (306)\n",
      "Collecting openai>=1.14.0 (from llama-index-agent-openai<0.3.0,>=0.1.4->llama-index)\n",
      "  Using cached openai-1.23.1-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting PyYAML>=6.0.1 (from llama-index-core<0.11.0,>=0.10.30->llama-index)\n",
      "  Using cached PyYAML-6.0.1-cp312-cp312-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting SQLAlchemy>=1.4.49 (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.30->llama-index)\n",
      "  Using cached SQLAlchemy-2.0.29-cp312-cp312-win_amd64.whl.metadata (9.8 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.6 (from llama-index-core<0.11.0,>=0.10.30->llama-index)\n",
      "  Using cached aiohttp-3.9.5-cp312-cp312-win_amd64.whl.metadata (7.7 kB)\n",
      "Collecting dataclasses-json (from llama-index-core<0.11.0,>=0.10.30->llama-index)\n",
      "  Using cached dataclasses_json-0.6.4-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.11.0,>=0.10.30->llama-index)\n",
      "  Using cached Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.11.0,>=0.10.30->llama-index)\n",
      "  Using cached dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting fsspec>=2023.5.0 (from llama-index-core<0.11.0,>=0.10.30->llama-index)\n",
      "  Using cached fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx (from llama-index-core<0.11.0,>=0.10.30->llama-index)\n",
      "  Using cached httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting llamaindex-py-client<0.2.0,>=0.1.18 (from llama-index-core<0.11.0,>=0.10.30->llama-index)\n",
      "  Using cached llamaindex_py_client-0.1.18-py3-none-any.whl.metadata (760 bytes)\n",
      "Collecting networkx>=3.0 (from llama-index-core<0.11.0,>=0.10.30->llama-index)\n",
      "  Using cached networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting nltk<4.0.0,>=3.8.1 (from llama-index-core<0.11.0,>=0.10.30->llama-index)\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting numpy (from llama-index-core<0.11.0,>=0.10.30->llama-index)\n",
      "  Using cached numpy-1.26.4-cp312-cp312-win_amd64.whl.metadata (61 kB)\n",
      "Collecting pandas (from llama-index-core<0.11.0,>=0.10.30->llama-index)\n",
      "  Using cached pandas-2.2.2-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Collecting pillow>=9.0.0 (from llama-index-core<0.11.0,>=0.10.30->llama-index)\n",
      "  Using cached pillow-10.3.0-cp312-cp312-win_amd64.whl.metadata (9.4 kB)\n",
      "Collecting tenacity<9.0.0,>=8.2.0 (from llama-index-core<0.11.0,>=0.10.30->llama-index)\n",
      "  Using cached tenacity-8.2.3-py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting tiktoken>=0.3.3 (from llama-index-core<0.11.0,>=0.10.30->llama-index)\n",
      "  Using cached tiktoken-0.6.0-cp312-cp312-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting tqdm<5.0.0,>=4.66.1 (from llama-index-core<0.11.0,>=0.10.30->llama-index)\n",
      "  Using cached tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.11.0,>=0.10.30->llama-index)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting wrapt (from llama-index-core<0.11.0,>=0.10.30->llama-index)\n",
      "  Using cached wrapt-1.16.0-cp312-cp312-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting beautifulsoup4<5.0.0,>=4.12.3 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\n",
      "  Using cached beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting pypdf<5.0.0,>=4.0.1 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\n",
      "  Using cached pypdf-4.2.0-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\n",
      "  Using cached striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting llama-parse<0.5.0,>=0.4.0 (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama-index)\n",
      "  Using cached llama_parse-0.4.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting PyJWT<3,>=1.0.0 (from PyJWT[crypto]<3,>=1.0.0->msal>=1.24.0->azure-identity)\n",
      "  Using cached PyJWT-2.8.0-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting portalocker<3,>=1.6 (from msal-extensions>=0.3.0->azure-identity)\n",
      "  Using cached portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.30->llama-index)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.30->llama-index)\n",
      "  Using cached attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.30->llama-index)\n",
      "  Using cached frozenlist-1.4.1-cp312-cp312-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.30->llama-index)\n",
      "  Using cached multidict-6.0.5-cp312-cp312-win_amd64.whl.metadata (4.3 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.30->llama-index)\n",
      "  Using cached yarl-1.9.4-cp312-cp312-win_amd64.whl.metadata (32 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\n",
      "  Using cached soupsieve-2.5-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting pycparser (from cffi>=1.12->cryptography>=2.5->azure-identity)\n",
      "  Using cached pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\moapurva\\documents\\github\\azure-search-vector-samples\\.venv\\lib\\site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel) (0.8.4)\n",
      "Collecting pydantic>=1.10 (from llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.30->llama-index)\n",
      "  Using cached pydantic-2.7.0-py3-none-any.whl.metadata (103 kB)\n",
      "Collecting anyio (from httpx->llama-index-core<0.11.0,>=0.10.30->llama-index)\n",
      "  Using cached anyio-4.3.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting certifi (from httpx->llama-index-core<0.11.0,>=0.10.30->llama-index)\n",
      "  Using cached certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting httpcore==1.* (from httpx->llama-index-core<0.11.0,>=0.10.30->llama-index)\n",
      "  Using cached httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting idna (from httpx->llama-index-core<0.11.0,>=0.10.30->llama-index)\n",
      "  Using cached idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting sniffio (from httpx->llama-index-core<0.11.0,>=0.10.30->llama-index)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.30->llama-index)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting click (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.30->llama-index)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.30->llama-index)\n",
      "  Using cached joblib-1.4.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.30->llama-index)\n",
      "  Using cached regex-2024.4.16-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai>=1.14.0->llama-index-agent-openai<0.3.0,>=0.1.4->llama-index)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\moapurva\\documents\\github\\azure-search-vector-samples\\.venv\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel) (0.2.13)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.21.0->azure-core<2.0.0,>=1.28.0->azure-search-documents)\n",
      "  Using cached charset_normalizer-3.3.2-cp312-cp312-win_amd64.whl.metadata (34 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.21.0->azure-core<2.0.0,>=1.28.0->azure-search-documents)\n",
      "  Using cached urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.30->llama-index)\n",
      "  Using cached greenlet-3.0.3-cp312-cp312-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.30->llama-index)\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.11.0,>=0.10.30->llama-index)\n",
      "  Using cached marshmallow-3.21.1-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting pytz>=2020.1 (from pandas->llama-index-core<0.11.0,>=0.10.30->llama-index)\n",
      "  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->llama-index-core<0.11.0,>=0.10.30->llama-index)\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\moapurva\\documents\\github\\azure-search-vector-samples\\.venv\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\moapurva\\documents\\github\\azure-search-vector-samples\\.venv\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\moapurva\\documents\\github\\azure-search-vector-samples\\.venv\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel) (0.2.2)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.30->llama-index)\n",
      "  Using cached annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.18.1 (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.30->llama-index)\n",
      "  Using cached pydantic_core-2.18.1-cp312-none-win_amd64.whl.metadata (6.7 kB)\n",
      "Using cached azure_search_documents-11.4.0-py3-none-any.whl (283 kB)\n",
      "Using cached llama_index-0.10.30-py3-none-any.whl (6.9 kB)\n",
      "Using cached llama_index_vector_stores_azureaisearch-0.1.4-py3-none-any.whl (8.1 kB)\n",
      "Using cached llama_index_embeddings_azure_openai-0.1.7-py3-none-any.whl (3.1 kB)\n",
      "Using cached azure_identity-1.16.0-py3-none-any.whl (166 kB)\n",
      "Using cached azure_common-1.1.28-py2.py3-none-any.whl (14 kB)\n",
      "Using cached azure_core-1.30.1-py3-none-any.whl (193 kB)\n",
      "Using cached cryptography-42.0.5-cp39-abi3-win_amd64.whl (2.9 MB)\n",
      "Using cached isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
      "Using cached llama_index_agent_openai-0.2.2-py3-none-any.whl (12 kB)\n",
      "Using cached llama_index_cli-0.1.12-py3-none-any.whl (26 kB)\n",
      "Using cached llama_index_core-0.10.30-py3-none-any.whl (15.4 MB)\n",
      "Using cached llama_index_embeddings_openai-0.1.7-py3-none-any.whl (6.0 kB)\n",
      "Using cached llama_index_indices_managed_llama_cloud-0.1.5-py3-none-any.whl (6.7 kB)\n",
      "Using cached llama_index_legacy-0.9.48-py3-none-any.whl (2.0 MB)\n",
      "Using cached llama_index_llms_azure_openai-0.1.6-py3-none-any.whl (4.8 kB)\n",
      "Using cached llama_index_llms_openai-0.1.15-py3-none-any.whl (10 kB)\n",
      "Using cached llama_index_multi_modal_llms_openai-0.1.5-py3-none-any.whl (5.8 kB)\n",
      "Using cached llama_index_program_openai-0.1.5-py3-none-any.whl (4.1 kB)\n",
      "Using cached llama_index_question_gen_openai-0.1.3-py3-none-any.whl (2.9 kB)\n",
      "Using cached llama_index_readers_file-0.1.19-py3-none-any.whl (36 kB)\n",
      "Using cached llama_index_readers_llama_parse-0.1.4-py3-none-any.whl (2.5 kB)\n",
      "Using cached msal-1.28.0-py3-none-any.whl (102 kB)\n",
      "Using cached msal_extensions-1.1.0-py3-none-any.whl (19 kB)\n",
      "Using cached aiohttp-3.9.5-cp312-cp312-win_amd64.whl (369 kB)\n",
      "Using cached beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "Using cached cffi-1.16.0-cp312-cp312-win_amd64.whl (181 kB)\n",
      "Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Using cached dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Using cached fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
      "Using cached llama_parse-0.4.1-py3-none-any.whl (7.3 kB)\n",
      "Using cached llamaindex_py_client-0.1.18-py3-none-any.whl (136 kB)\n",
      "Using cached httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "Using cached httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "Using cached networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached openai-1.23.1-py3-none-any.whl (310 kB)\n",
      "Using cached pillow-10.3.0-cp312-cp312-win_amd64.whl (2.5 MB)\n",
      "Using cached portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
      "Using cached PyJWT-2.8.0-py3-none-any.whl (22 kB)\n",
      "Using cached pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
      "Using cached PyYAML-6.0.1-cp312-cp312-win_amd64.whl (138 kB)\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Using cached SQLAlchemy-2.0.29-cp312-cp312-win_amd64.whl (2.1 MB)\n",
      "Using cached striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
      "Using cached tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Using cached tiktoken-0.6.0-cp312-cp312-win_amd64.whl (798 kB)\n",
      "Using cached tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "Using cached typing_extensions-4.11.0-py3-none-any.whl (34 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached wrapt-1.16.0-cp312-cp312-win_amd64.whl (37 kB)\n",
      "Using cached dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
      "Using cached numpy-1.26.4-cp312-cp312-win_amd64.whl (15.5 MB)\n",
      "Using cached pandas-2.2.2-cp312-cp312-win_amd64.whl (11.5 MB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached anyio-4.3.0-py3-none-any.whl (85 kB)\n",
      "Using cached attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "Using cached certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp312-cp312-win_amd64.whl (100 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached frozenlist-1.4.1-cp312-cp312-win_amd64.whl (50 kB)\n",
      "Using cached greenlet-3.0.3-cp312-cp312-win_amd64.whl (293 kB)\n",
      "Using cached idna-3.7-py3-none-any.whl (66 kB)\n",
      "Using cached marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
      "Using cached multidict-6.0.5-cp312-cp312-win_amd64.whl (27 kB)\n",
      "Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Using cached pydantic-2.7.0-py3-none-any.whl (407 kB)\n",
      "Using cached pydantic_core-2.18.1-cp312-none-win_amd64.whl (1.9 MB)\n",
      "Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Using cached regex-2024.4.16-cp312-cp312-win_amd64.whl (268 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached soupsieve-2.5-py3-none-any.whl (36 kB)\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Using cached urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "Using cached yarl-1.9.4-cp312-cp312-win_amd64.whl (76 kB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached joblib-1.4.0-py3-none-any.whl (301 kB)\n",
      "Using cached pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Using cached annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: striprtf, pytz, dirtyjson, azure-common, wrapt, urllib3, tzdata, typing-extensions, tqdm, tenacity, soupsieve, sniffio, regex, PyYAML, pypdf, PyJWT, pycparser, portalocker, pillow, numpy, networkx, mypy-extensions, multidict, marshmallow, joblib, isodate, idna, h11, greenlet, fsspec, frozenlist, distro, click, charset-normalizer, certifi, attrs, annotated-types, yarl, typing-inspect, SQLAlchemy, requests, pydantic-core, pandas, nltk, httpcore, deprecated, cffi, beautifulsoup4, anyio, aiosignal, tiktoken, pydantic, httpx, dataclasses-json, cryptography, azure-core, aiohttp, openai, llamaindex-py-client, azure-search-documents, msal, llama-index-legacy, llama-index-core, msal-extensions, llama-parse, llama-index-vector-stores-azureaisearch, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, azure-identity, llama-index-program-openai, llama-index-llms-azure-openai, llama-index-question-gen-openai, llama-index-embeddings-azure-openai, llama-index\n",
      "Successfully installed PyJWT-2.8.0 PyYAML-6.0.1 SQLAlchemy-2.0.29 aiohttp-3.9.5 aiosignal-1.3.1 annotated-types-0.6.0 anyio-4.3.0 attrs-23.2.0 azure-common-1.1.28 azure-core-1.30.1 azure-identity-1.16.0 azure-search-documents-11.4.0 beautifulsoup4-4.12.3 certifi-2024.2.2 cffi-1.16.0 charset-normalizer-3.3.2 click-8.1.7 cryptography-42.0.5 dataclasses-json-0.6.4 deprecated-1.2.14 dirtyjson-1.0.8 distro-1.9.0 frozenlist-1.4.1 fsspec-2024.3.1 greenlet-3.0.3 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 idna-3.7 isodate-0.6.1 joblib-1.4.0 llama-index-0.10.30 llama-index-agent-openai-0.2.2 llama-index-cli-0.1.12 llama-index-core-0.10.30 llama-index-embeddings-azure-openai-0.1.7 llama-index-embeddings-openai-0.1.7 llama-index-indices-managed-llama-cloud-0.1.5 llama-index-legacy-0.9.48 llama-index-llms-azure-openai-0.1.6 llama-index-llms-openai-0.1.15 llama-index-multi-modal-llms-openai-0.1.5 llama-index-program-openai-0.1.5 llama-index-question-gen-openai-0.1.3 llama-index-readers-file-0.1.19 llama-index-readers-llama-parse-0.1.4 llama-index-vector-stores-azureaisearch-0.1.4 llama-parse-0.4.1 llamaindex-py-client-0.1.18 marshmallow-3.21.1 msal-1.28.0 msal-extensions-1.1.0 multidict-6.0.5 mypy-extensions-1.0.0 networkx-3.3 nltk-3.8.1 numpy-1.26.4 openai-1.23.1 pandas-2.2.2 pillow-10.3.0 portalocker-2.8.2 pycparser-2.22 pydantic-2.7.0 pydantic-core-2.18.1 pypdf-4.2.0 pytz-2024.1 regex-2024.4.16 requests-2.31.0 sniffio-1.3.1 soupsieve-2.5 striprtf-0.0.26 tenacity-8.2.3 tiktoken-0.6.0 tqdm-4.66.2 typing-extensions-4.11.0 typing-inspect-0.9.0 tzdata-2024.1 urllib3-2.2.1 wrapt-1.16.0 yarl-1.9.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find platform independent libraries <prefix>\n"
     ]
    }
   ],
   "source": [
    "####Install required packages\n",
    "%pip install ipykernel azure-search-documents llama-index llama-index-vector-stores-azureaisearch llama-index-embeddings-azure-openai azure-identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modules\n",
    "import os\n",
    "from IPython.display import Markdown, display\n",
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.vector_stores.azureaisearch import AzureAISearchVectorStore, IndexManagement, MetadataIndexFieldType\n",
    "from llama_index.core import StorageContext, VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core import Settings\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.core.credentials import AzureKeyCredential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Import Azure Open AI Variables\n",
    "##Obtain Azure OpenAI (AOAI) key & endpoint in the Azure Portal -> Azure OpenAI -> AOAI Instance -> Resource Management -> Keys and Endpoint\n",
    "AOAI_API_KEY = 'your alphanumeric AOAI key'\n",
    "AOAI_ENDPOINT = 'https://youraoai.openai.azure.com/'\n",
    "#AOAI api version below should work for gpt-4 models\n",
    "AOAI_API_VERSION = '2024-02-15-preview'\n",
    "#Change if you want to use a different model, ensure you use the 'model name' and not the 'deployment name' from the Azure OpenAI Service\n",
    "AOAI_LLM_MODEL = 'gpt-4'\n",
    "#Change the deployment name of your AOAI llm model if not the same as the model name\n",
    "AOAI_LLM_DEPLOYMENT_NAME = \"gpt-4\"\n",
    "#The Azure OpenAI Embedding model is a separate model that is used to convert text to embeddings, usually text-embedding-ada-002\n",
    "AOAI_EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "#Change to the 'deployment name' of your embeddings model if not same as the model name\n",
    "AOAI_EMBEDDING_DEPLOYMENT_NAME = \"text-embedding-ada-002\"\n",
    "\n",
    "####Create Azure AI Search Variables\n",
    "##Obtain from Azure Portal -> AI Search -> Settings -> Keys\n",
    "SEARCH_SERVICE_API_KEY = \"your AI Search key\"\n",
    "#Obtain from Azure Portal -> AI Search -> Overview -> Url \n",
    "SEARCH_SERVICE_ENDPOINT = \"https://youraisearchurl.search.windows.net\"\n",
    "#If needed to change, versions table here: https://learn.microsoft.com/en-us/rest/api/searchservice/search-service-api-versions#stable-versions\n",
    "SEARCH_SERVICE_API_VERSION = \"2023-11-01\"\n",
    "CREDENTIAL = AzureKeyCredential(SEARCH_SERVICE_API_KEY)\n",
    "#Name the index to be created in AI Search (lowercase, digits or dashes only)\n",
    "INDEX_NAME = \"llamaindex-vector-demo\"\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "    model=AOAI_LLM_MODEL,\n",
    "    deployment_name=AOAI_LLM_DEPLOYMENT_NAME,\n",
    "    api_key=AOAI_API_KEY,\n",
    "    azure_endpoint=AOAI_ENDPOINT,\n",
    "    api_version=AOAI_API_VERSION,\n",
    ")\n",
    "\n",
    "# You need to deploy your own embedding model as well as your own chat completion model\n",
    "embed_model = AzureOpenAIEmbedding(\n",
    "    model=AOAI_EMBEDDING_MODEL,\n",
    "    deployment_name=AOAI_EMBEDDING_DEPLOYMENT_NAME,\n",
    "    api_key=AOAI_API_KEY,\n",
    "    azure_endpoint=AOAI_ENDPOINT,\n",
    "    api_version=AOAI_API_VERSION,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Setup AI Search\n",
    "\n",
    "# Use index client to demonstrate creating an index\n",
    "index_client = SearchIndexClient(\n",
    "    endpoint=SEARCH_SERVICE_ENDPOINT,\n",
    "    credential=CREDENTIAL,\n",
    ")\n",
    "\n",
    "# Use search client to demonstration using existing index\n",
    "search_client = SearchClient(\n",
    "    endpoint=SEARCH_SERVICE_ENDPOINT,\n",
    "    index_name=INDEX_NAME,\n",
    "    credential=CREDENTIAL,\n",
    ")\n",
    "\n",
    "metadata_fields = {\n",
    "    \"author\": \"author\",\n",
    "    \"theme\": (\"topic\", MetadataIndexFieldType.STRING),\n",
    "    \"director\": \"director\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "###vector store configuration\n",
    "vector_store = AzureAISearchVectorStore(\n",
    "    search_or_index_client=index_client,\n",
    "    filterable_metadata_field_keys=metadata_fields,\n",
    "    index_name=INDEX_NAME,\n",
    "    index_management=IndexManagement.CREATE_IF_NOT_EXISTS,\n",
    "    id_field_key=\"id\",\n",
    "    chunk_field_key=\"chunk\",\n",
    "    embedding_field_key=\"embedding\",\n",
    "    embedding_dimensionality=1536,\n",
    "    metadata_string_field_key=\"metadata\",\n",
    "    doc_id_field_key=\"doc_id\",\n",
    "    language_analyzer=\"en.lucene\",\n",
    "    vector_algorithm_type=\"exhaustiveKnn\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Download and index the sample dataset to the working directory\n",
    "import requests\n",
    "url = 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt'\n",
    "response = requests.get(url)\n",
    "with open('paul_graham_essay.txt', 'wb') as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "reader = SimpleDirectoryReader(input_files=[\"paul_graham_essay.txt\"])\n",
    "documents = reader.load_data()\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The author worked on several different things throughout his life. Before college, he worked on writing short stories and programming. His first programming experiences were with an IBM 1401 using punch cards and Fortran. Later, he worked with microcomputers, writing simple games, a program to predict model rocket flights, and a word processor his father used for writing a book.\n",
       "\n",
       "In college, he initially planned to study philosophy but switched to AI, inspired by a novel by Heinlein and a PBS documentary featuring SHRDLU. He also worked on Lisp, creating a new dialect called Arc, and wrote essays that he published online, which gained significant attention after being shared on Slashdot. He wrote a book about Lisp hacking titled \"On Lisp,\" and engaged in systems work, desiring to build things that would last.\n",
       "\n",
       "Additionally, he worked on spam filters, did some painting, hosted dinners, and bought a building in Cambridge. He also contemplated a career in art after realizing paintings could be a lasting form of creation, taking art classes while still in a PhD program for computer science. Ultimately, he completed his dissertation in a short time frame after being asked if he could graduate that June."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Query Data - First query can take up to 5 minutes depending on the size of the dataset to index\n",
    "query_engine = index.as_query_engine(similarity_top_k=3)\n",
    "response = query_engine.query(\"What did the author work on?\")\n",
    "display(Markdown(f\"{response}\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
